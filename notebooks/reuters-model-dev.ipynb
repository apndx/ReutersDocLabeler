{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Reuters corpus topic classification\n",
    "\n",
    "This project is about topic classification on the Reuters corpus. It is multi-label classification: there can be more than one topics associated with each document.\n",
    "\n",
    "### Data\n",
    "\n",
    "Test data will be extracted from XML-documents, taking input from <headline></headline> and <text></text>, target classes from <codes class = 'bip:topics:1.0'><code code = \"topic_i\"></code></codes>\n",
    "\n",
    "input: 'document text string, each row a document'\n",
    "target: ['topic_1', '...', 'topic_n'] = [0, ...., 1, 0]\n",
    "\n",
    "\n",
    "There are 126 topics that are listed in the topic_codes.txt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import *\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "First I load the data, and because of the target and codes are stored as strings and not as list, I will convert them back to lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trunc_large_data_strings = pd.read_csv('reuters-csv/inputs_trunc.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trunc_large_data = trunc_large_data_strings.copy(deep=True)\n",
    "trunc_large_data['target'] = trunc_large_data['target'].apply(eval)\n",
    "trunc_large_data['codes'] = trunc_large_data['codes'].apply(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out transformer and BERT\n",
    "\n",
    "Next I will be trying out things presented in a blog post: [Transformers for Multi-Label Classification made simple.](https://towardsdatascience.com/transformers-for-multilabel-classification-71a1a0daf5e1)\n",
    "\n",
    "I will also utilize some code from the home exercises of Deep Learning course.\n",
    "\n",
    "#### Data splits and dataloaders\n",
    "\n",
    "Let's split the data and make iterators for batch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to lists\n",
    "\n",
    "documents = list(trunc_large_data.text.values)\n",
    "labels = list(trunc_large_data.target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) \n",
    "\n",
    "# encodings\n",
    "\n",
    "encodings = tokenizer.batch_encode_plus(documents, padding='max_length', truncation=True) # tokenizer's encoding method\n",
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "token_type_ids = encodings['token_type_ids'] # token type ids\n",
    "attention_masks = encodings['attention_mask'] # attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take rows that have too rare targets\n",
    "\n",
    "label_counts = trunc_large_data.target.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(trunc_large_data[trunc_large_data.target.astype(str).isin(one_freq)].index), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4596"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(one_freq_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering single instance inputs\n",
    "one_freq_input_ids = [input_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_token_types = [token_type_ids.pop(i) for i in one_freq_idxs]\n",
    "one_freq_attention_masks = [attention_masks.pop(i) for i in one_freq_idxs]\n",
    "one_freq_labels = [labels.pop(i) for i in one_freq_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295177"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data is 299773 rows long, and 295177 when the unique target occurances have been separated. I will take 3 % split of these rows to mini train, and split dev and test from the remained part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_inputs, mini_train_inputs, remaining_labels, mini_train_labels, remaining_token_types, mini_train_token_types, remaining_masks, mini_train_masks = train_test_split(input_ids, labels, token_type_ids,attention_masks,\n",
    "                                                            random_state=42, test_size=0.03, stratify = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remaining:  286321 mini-train:  8856\n"
     ]
    }
   ],
   "source": [
    "print('remaining: ', len(remaining_inputs), 'mini-train: ', len(mini_train_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset further, now getting the dev mini set\n",
    "\n",
    "remaining_inputs2, mini_dev_inputs, remaining_labels2, mini_dev_labels, remaining_token_types2, mini_dev_token_types, remaining_masks2, mini_dev_masks = train_test_split(remaining_inputs, remaining_labels, remaining_token_types, remaining_masks,\n",
    "                                                            random_state=42, test_size=0.03, stratify = remaining_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remaining2:  277731 mini-dev:  8590\n"
     ]
    }
   ],
   "source": [
    "print('remaining2: ', len(remaining_inputs2), 'mini-dev: ', len(mini_dev_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and then splitting the test set\n",
    "\n",
    "remaining_inputs3, mini_test_inputs, remaining_labels3, mini_test_labels, remaining_token_types3, mini_test_token_types, remaining_masks3, mini_test_masks = train_test_split(remaining_inputs2, remaining_labels2, remaining_token_types2, remaining_masks2,\n",
    "                                                            random_state=42, test_size=0.03, stratify = remaining_labels2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remaining3:  269399 mini-dev:  8332\n"
     ]
    }
   ],
   "source": [
    "print('remaining3: ', len(remaining_inputs3), 'mini-dev: ', len(mini_test_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will initially include all the 4596 one_freq rows in the mini_train_data, this can be changed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_train_inputs.extend(one_freq_input_ids)\n",
    "mini_train_labels.extend(one_freq_labels)\n",
    "mini_train_masks.extend(one_freq_attention_masks)\n",
    "mini_train_token_types.extend(one_freq_token_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change mini train sets to tensors\n",
    "\n",
    "t_mini_train_inputs = torch.tensor(mini_train_inputs)\n",
    "t_mini_train_labels = torch.tensor(mini_train_labels)\n",
    "t_mini_train_masks = torch.tensor(mini_train_masks)\n",
    "t_mini_train_token_types = torch.tensor(mini_train_token_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change mini dev sets to tensors\n",
    "\n",
    "t_mini_dev_inputs = torch.tensor(mini_dev_inputs)\n",
    "t_mini_dev_labels = torch.tensor(mini_dev_labels)\n",
    "t_mini_dev_masks = torch.tensor(mini_dev_masks)\n",
    "t_mini_dev_token_types = torch.tensor(mini_dev_token_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change mini test sets to tensors\n",
    "\n",
    "t_mini_test_inputs = torch.tensor(mini_test_inputs)\n",
    "t_mini_test_labels = torch.tensor(mini_test_labels)\n",
    "t_mini_test_masks = torch.tensor(mini_test_masks)\n",
    "t_mini_test_token_types = torch.tensor(mini_test_token_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13452, 512])\n",
      "torch.Size([13452, 126])\n",
      "torch.Size([13452, 512])\n",
      "torch.Size([13452, 512])\n"
     ]
    }
   ],
   "source": [
    "print(t_mini_train_inputs.shape)\n",
    "print(t_mini_train_labels.shape)\n",
    "print(t_mini_train_masks.shape)\n",
    "print(t_mini_train_token_types.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train iterator with torch dataloader\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "mini_train_data = TensorDataset(t_mini_train_inputs, t_mini_train_masks, t_mini_train_labels, t_mini_train_token_types)\n",
    "mini_train_sampler = RandomSampler(mini_train_data)\n",
    "mini_train_dataloader = DataLoader(mini_train_data, sampler=mini_train_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dev iterator with torch dataloader\n",
    "\n",
    "mini_dev_data = TensorDataset(t_mini_dev_inputs, t_mini_dev_masks, t_mini_dev_labels, t_mini_dev_token_types)\n",
    "mini_dev_sampler = SequentialSampler(mini_dev_data)\n",
    "mini_dev_dataloader = DataLoader(mini_dev_data, sampler=mini_dev_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test iterator with torch dataloader\n",
    "\n",
    "mini_test_data = TensorDataset(t_mini_test_inputs, t_mini_test_masks, t_mini_test_labels, t_mini_test_token_types)\n",
    "mini_test_sampler = SequentialSampler(mini_test_data)\n",
    "mini_test_dataloader = DataLoader(mini_test_data, sampler=mini_test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save (file sizes are around 95-180 MB for mini loaders, so these are gitignored)\n",
    "\n",
    "torch.save(mini_train_dataloader,'data-loaders/mini_train_data_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(mini_dev_dataloader,'data-loaders/mini_dev_data_loader')\n",
    "torch.save(mini_test_dataloader,'data-loaders/mini_test_data_loader')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut\n",
    "\n",
    "If there is a saved dataloader, this can be the startpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_mini_train_dataloader = torch.load('data-loaders/mini-train_data_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hdheli/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading, initial optimizer and criterion\n",
    "\n",
    "The huggingface transformers have AdamW that has gradient bias correction and weight decay. The optimizer hyperparemeters can be [customised](https://huggingface.co/transformers/training.html), but I start simple with the default settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Model 1\n",
    "\n",
    "NUM_LABELS = 126 # amount of the different topics\n",
    "ADAM_DEFAULT_LR = 1e-5\n",
    "\n",
    "model_1 = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=NUM_LABELS)\n",
    "model_1.to(device)\n",
    "\n",
    "optimizer_1 = AdamW(model_1.parameters(), lr=ADAM_DEFAULT_LR)\n",
    "criterion_1 = BCEWithLogitsLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train Loop ---\n",
    "\n",
    "import time\n",
    "\n",
    "def train_loop(model, model_name, optimizer, criterion, n_epochs, dataloader):\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    steps = 0\n",
    "    examples = 0\n",
    "    all_batch_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "        batch_losses = []\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            batch_start_time = time.time()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels, b_token_types = batch # unpack from dataloader\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            logits = outputs[0]\n",
    "            loss = criterion(logits.view(-1, NUM_LABELS),b_labels.type_as(logits).view(-1,NUM_LABELS)) #convert labels to float for calculation\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            examples += b_input_ids.size(0)\n",
    "            steps += 1\n",
    "            batch_end_time = time.time() \n",
    "            \n",
    "            # Loss check\n",
    "            loss_check = epoch_loss/(step+1)\n",
    "            batch_losses.append(loss_check)\n",
    "            batch_mins, batch_secs = epoch_time(batch_start_time, batch_end_time)\n",
    "            print(f'Epoch: {epoch+1:02} | Step {step} | Batch time: {batch_mins}m {batch_secs}s')\n",
    "            print(f'\\tLoss check: {loss_check:.3f}')\n",
    "        \n",
    "        torch.save(model.state_dict(), model_name)    \n",
    "        train_loss = epoch_loss / len(dataloader)\n",
    "        train_losses.append(train_loss)\n",
    "        all_batch_losses.append(batch_losses)\n",
    "            \n",
    "        end_time = time.time()\n",
    "            \n",
    "        epoch_mins, epoch_secs = epoch_time(batch_start_time, batch_end_time)\n",
    "        print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "        \n",
    "            \n",
    "    return model, train_losses, all_batch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model 1, just one epoch to test\n",
    "\n",
    "n_epochs_1 = 1\n",
    "\n",
    "trained_model_1, train_losses, batch_losses = train_loop(model_1, OUTPUT_FOLDER+'/model_1_270321', optimizer_1, criterion_1, n_epochs_1, loaded_mini_train_dataloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
